
##########################################################################################################
					Kubernetes Networking
##########################################################################################################
References: 
https://sookocheff.com/post/kubernetes/understanding-kubernetes-networking-model/

			
			
			Written in 2018
			---------------


Kubernetes 
	built to run distributed systems over a cluster of machines. 
	Distributed: Networking a central and necessary component of Kubernetes deployment
		understanding the Kubernetes networking model will allow you to correctly 
			run, 
			monitor and 
			troubleshoot 
			your applications running on Kubernetes.

	
	The Kubernetes Networking Model
	-------------------------------
	Kubernetes makes opinionated choices about how Pods are networked. 
	Kubernetes dictates the following requirements on any networking implementation:
		1. All Pods can communicate with all other Pods without using network address translation (NAT).
		2. All Nodes can communicate with all Pods without NAT.
		3. The IP that a Pod sees itself as is the same IP that others see it as.
		
	So four distinct networking problems needs to be solved for this:
		1. Container-to-Container networking
		2. Pod-to-Pod networking
		3. Pod-to-Service networking
		4. Internet-to-Service networking



Network Namespace
-----------------
In Linux, 
	each running process communicates within a network namespace 
		that provides a logical networking stack with its own routes, firewall rules, and network devices. 
	i.e. a network namespace provides a brand new network stack for all the processes within the namespace.

	create network namespaces - ns1
		$ ip netns add ns1

	When the namespace is created
		a mount point for it is created under 
			/var/run/netns 
			(Default this folder may not be present as there are no n/w ns)
						
		allowing the namespace to persist even if there is no process attached to it.

	list available namespaces by listing all the mount points under /var/run/netns, or by using the ip command.
		$ ls /var/run/netns
			ns1
		or 
		$ ip netns
			ns1
			
	The root network namespace.
	--------------------------
	Linux assigns every process to the root network namespace to provide access to the external world


	In terms of Docker constructs
		Pod is modelled as a group of Docker containers that share a network namespace. 
		Containers within a Pod all have the same IP address and port space 
			assigned through the network namespace assigned to the Pod
				(docker's netns is not found on /var/run/netns)
		Can find each other via localhost 
			since they reside in the same namespace. 
		We can create a network namespace for each Pod on a virtual machine
		This is implemented, using Docker
			as a “Pod container” which holds the network namespace open while “app containers” 
				(the things the user specified) 
				join that namespace with Docker’s –net=container: function. 
		
		How is that possible?
			https://blog.mikesir87.io/2019/03/sharing-network-namespaces-in-docker/
			
	Applications within a Pod also have access to shared volumes, 
	which are defined as part of a Pod and are made available to be mounted into each application’s filesystem.


	Pod-to-Pod Networking
	---------------------
	Every Pod has a real IP address 
	Pod communicates with other Pods using that IP address. 
	
	How Kubernetes enables Pod-to-Pod communication using real IPs?
	Whether the Pod is deployed on the same physical Node or different Node in the cluster?
	
	Pods that reside on the same machine: the Pod’s perspective
	-----------------------------------------------------------
	It exists in its own Ethernet namespace 
		Needs to communicate with other network namespaces on the same Node. 
	
	Namespaces can be connected using a Linux Virtual Ethernet Device called veth pair 	
	
	Veth pair:
		To connect Pod namespaces
			we can assign one side of the veth pair to the root network namespace
			and the other side to the Pod’s network namespace. 
		Each veth pair works like a patch cable
			connecting the two sides and allowing traffic to flow between them. 
		This setup can be replicated for as many Pods as we have on the machine. 


	Now 
		Pods: 
			have their one network namespace 
			they have their own Ethernet device and IP address
			they are connected to the root namespace for the Node. 
	
	Network Bridge
		Now, we want the Pods to talk to each other through the root namespace
			for this we use a network bridge.

	A Linux Ethernet bridge 
		Virtual Layer 2 networking device 
		Used to unite two or more network segments
		Work transparently to connect two networks together. 
		The bridge operates by 
			maintaining a forwarding table between sources and destinations 
			Examines the destination of the data packets that travel through it 
			Decides whether or not to pass the packets to other network segments connected to the bridge. 
			
			The bridging code 
				decides whether to bridge data or 
				to drop it by looking at the MAC-address 
					unique to each Ethernet device in the network.

	
	Bridges implement the ARP protocol 
		discover the link-layer MAC address associated with a given IP address. 
		
		When a data frame is received at the bridge, 
			bridge broadcasts the frame out to all connected devices (except the original sender) 
			device that responds to the frame is stored in a lookup table. 
			Future traffic with the same IP address uses the 
			lookup table to discover the correct MAC address to forward the packet to.

	4.1 Life of a packet: Pod-to-Pod, same Node
	-------------------------------------------
	Given the network namespaces that isolate each Pod to their own networking stack, 
		virtual Ethernet devices that connect each namespace to the root namespace, 
		bridge that connects namespaces together, we can send traffic between Pods on the same Node. 
		
		Pod 1 sends a packet to its own Ethernet device eth0 
			available as the default device. 
			For Pod 1, eth0 is connected via a virtual Ethernet device to the root namespace, veth0 (1). 
			The bridge cbr0 is configured with veth0 a network segment attached to it. 
			Once the packet reaches the bridge, 
				the bridge resolves the correct network segment to send the packet to — 
					veth1 using the ARP protocol. 
		When the packet reaches the virtual device veth1, 
			it is forwarded directly to Pod 2’s namespace 
			eth0 device within that namespace. 
		Throughout this traffic flow, 
			each Pod is communicating only with eth0 on localhost 
			traffic is routed to the correct Pod. 
			The development experience for using the network is the default behaviour that a developer would expect.

Kubernetes networking model dictates 
	Pods must be reachable by their IP address across Nodes. 

i.e, 
	The IP address of a Pod is always visible to other Pods in the network
	Each Pod views its own IP address as the same as how other Pods see it
	
	Routing traffic between Pods on different Nodes
	-----------------------------------------------

		Life of a packet: Pod-to-Pod, across Nodes
		------------------------------------------

	Kubernetes networking model:
		Pod IPs are reachable across the network
			how? 
				Each network plugin implementation can decide.
				
			but some patterns have been established to make this easier.

Generally
	Every Node in your cluster is assigned a CIDR block 
		specifying the IP addresses available to Pods running on that Node. 
		
	Once traffic destined for the CIDR block 
		reaches the Node 
			it is the Node’s responsibility to forward traffic to the correct Pod. 
			

	the destination Pod is on a different Node from the source Pod. 
	The packet begins by being sent through Pod 1’s Ethernet device which is paired with the virtual Ethernet device in the root namespace 
	Ultimately, the packet ends up at the root namespace’s network bridge 
	
	ARP will fail at the bridge because there is no device connected to the bridge with the correct MAC address for the packet. 
		ARP defines the rout at the bridge
	On failure, 
		the bridge sends the packet out the default route — 
			the root namespace’s eth0 device. 
			At this point the route leaves the Node and enters the network 
	We assume for now that the network can route the packet to the correct Node based on the CIDR block assigned to the Node 
	The packet enters the root namespace of the destination Node (eth0 on VM 2), 
		it is routed through the bridge to the correct virtual Ethernet device 
	Route completes by flowing through the virtual Ethernet device’s pair residing within Pod’s namespace 
	Generally speaking, 
		each Node knows how to deliver packets to Pods that are running within it. 
	Once a packet reaches a destination Node, 
		packets flow the same way they do for routing traffic between Pods on the same Node.

	
	
	Now how to configure the network to route traffic for Pod IPs to the correct Node that is responsible for those IPs? 
	This is network specific, 
		Let's look at a specific example to understand one implementation. 
	With AWS, 
		Amazon maintains a container networking plugin for Kubernetes 
		This allows Node to Node networking to operate within an Amazon VPC environment using
			[Container Networking Interface (CNI) plugin] 
			(https://github.com/aws/amazon-vpc-cni-k8s).

	
	Container Networking Interface (CNI) 
	------------------------------------
	Provides a common API for connecting containers to the outside network. 
	Pod can communicate with the network using IP addresses
	All CNI plugin should do this.
	
---------------------------------------------------------------------------------------------------------------------
			AWS	Specific

	AWS also implements the plugin meeting the k8s expectation along with providing the following capabilities.
		secure 
		manageable environment 
		through the existing 
		VPC, 
		IAM, and 
		Security Group functionality 
		provided by AWS. 
		
		Done using elastic network interfaces.


	In EC2, 
		each instance is bound to an elastic network interface (ENI) 
		All ENIs are connected within a VPC 
		ENIs are able to reach each other without additional effort. 
		By default, 
			each EC2 instance is deployed with a single ENI, 
			but you are free to create multiple ENIs and deploy them to EC2 instances. 
		The AWS CNI plugin for Kubernetes leverages this flexibility by creating a new ENI for each Pod deployed to a Node. 
		ENIs in a VPC are already connected within the existing AWS infrastructure
			this allows each Pod’s IP address to be natively addressable within the VPC. 
		When the CNI plugin is deployed to the cluster, 
			each Node (EC2 instance) creates multiple elastic network interfaces and allocates IP addresses to those instances
				forming a CIDR block for each Node. 
		When Pods are deployed
			small binary deployed to the Kubernetes cluster as a DaemonSet receives any requests to add a Pod 
				to the network from the Nodes local kubelet process. 
			This binary picks an available IP address from the Node’s available pool of ENIs and assigns it to the Pod 
				by wiring the virtual Ethernet device and bridge within the Linux kernel as described when networking Pods within the same Node. 
				With this in place, Pod traffic is routeable across Nodes within the cluster.
---------------------------------------------------------------------------------------------------------------------


		Pod-to-Service Networking
		-------------------------
	Pod's can go down and come with with a different IP address.

	Pod IP addresses 
		not durable 
		will appear and disappear in response to 
			scaling out or in, 
			application crashes, 
			Node reboots. 
	Services solves this.
	
	A Kubernetes Service 
	--------------------
		manages the state of a set of Pods, 
		allowing you to track a set of Pod IP addresses 
		that are dynamically changing over time. 
		
	Services act as an abstraction over Pods 
	Assign a single virtual IP address to a group of Pod IP addresses. 
	
	Any traffic addressed to the virtual IP of the Service 
		will be routed to the set of Pods that are associated with the virtual IP. 
	This allows the set of Pods associated with a Service to change at any time 
		clients only need to know the Service’s virtual IP, which does not change.


		When creating a new Kubernetes Service
		--------------------------------------
		A new virtual IP (also known as a cluster IP) is created on your behalf. 
		Anywhere within the cluster, 
			traffic addressed to the virtual IP will be load-balanced to the set of backing Pods associated with the Service. 
		Effectively, 
			Kubernetes automatically creates and maintains a distributed in-cluster load balancer 
				that distributes traffic to a Service’s associated healthy Pods. 

		netfilter and iptables
		----------------------
		To perform load balancing within the cluster
			Kubernetes relies on the networking framework built in to Linux 
				netfilter. 
			Netfilter is a framework provided by Linux 
			That allows various networking-related operations to be implemented in the form of customized handlers. 
			Netfilter offers various functions and operations for 
				packet filtering
				NAT ing
				port translation
			provides the functionality required for directing packets through a network
			prohibit packets from reaching sensitive locations within a computer network.

		iptables is a user-space program 
			providing a table-based system for defining rules for 
				manipulating and transforming packets using the netfilter framework.
				
			Role of kube-proxy
			------------------
			In Kubernetes, 
				Iptables rules are configured by the kube-proxy controller 
				Watches the Kubernetes API server for changes. 
				
				NAT'ing
				-------
				When a change to a Service or Pod 
					updates the virtual IP address of the Service or 
					IP address of a Pod
				iptables rules are updated to correctly route traffic directed at a Service to a backing Pod. 
				
				The iptables rules watch for traffic destined for a Service’s virtual IP 
					on a match, 
					a random Pod IP address is selected from the set of available Pods 
				iptables rule changes the packet’s destination IP address 
					from the Service’s virtual IP 
					to the IP of the selected Pod. 			
				As Pods come up or go down, 
					the iptables ruleset is updated 
						to reflect the changing state of the cluster. 
				Indirectly
					iptables has done load-balancing on the machine 
						to take traffic directed to a service’s IP to an actual pod’s IP.

				DNAT'ing
				--------
				On the return path, 
					the IP address is coming from the destination Pod. 
				In this case iptables again rewrites the IP header to replace the Pod IP 
					with the Service’s IP so that the Pod believes 
						it has been communicating solely with the Service’s IP the entire time.

		IPVS
		----
		Since v 1.11 
			Kubernetes includes a second option for in-cluster load balancing: 
				IPVS. 
			IPVS (IP Virtual Server) 
				Built on top of netfilter 
				Implements transport-layer load balancing as part of the Linux kernel. 
			IPVS is incorporated into the LVS (Linux Virtual Server)
			It runs on a host
			Acts as a load balancer in front of a cluster of real servers. 
			IPVS can direct requests for TCP - and UDP-based services 
				to the real servers, 
				make services of the real servers appear as virtual services 
					on a single IP address. 
			This makes IPVS a natural fit for Kubernetes Services.

		While declaring Kubernetes Service
			we can decide to use in-cluster load balancing using
				iptables or 
				IPVS. 
			IPVS is specifically designed for load balancing 
			Uses more efficient data structures (hash tables)
			Allowing for almost unlimited scale compared to iptables. 
		
		When creating a Service load balanced with IPVS
			three things happen: 
				a dummy IPVS interface is created on the Node, 
				Service’s IP address is bound to the dummy IPVS interface
				IPVS servers are created for each Service IP address.

		Currently round robin is the default load-balancing. 
		This can be modified using --ipvs-scheduler
			https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/
			
		
		Life of a packet through an in-cluster load-balanced Service
		------------------------------------------------------------

		Life of a packet: Pod to Service
		--------------------------------
		When routing a packet between a Pod and Service
			The packet leaves the Pod through the eth0 interface attached to the Pod’s network namespace 
			Then it travels through the virtual Ethernet device to the bridge 
			The ARP protocol running on the bridge does not know about the Service 
				so it transfers the packet out through the default route — eth0
			Here, something different happens. 
		
		Before being accepted at eth0, 
			the packet is filtered through iptables. 
		After receiving the packet, 
			iptables uses the rules installed on the Node by kube-proxy 
				in response to Service or Pod events 
				to rewrite the destination of the packet 
					from the Service IP 
					to a specific Pod IP 
		The packet is now destined to reach Pod 4 rather than the Service’s virtual IP. 
		
		The Linux kernel’s conntrack utility is leveraged by iptables to 
			remember the Pod choice that was made so future traffic is routed to the same Pod 
				(barring any scaling events). 
		In essence, 
			iptables has done in-cluster load balancing 
				directly on the Node. 
		Traffic then flows to the Pod 
			using the Pod-to-Pod routing we’ve already examined section 5.

		Life of a packet: Service to Pod
		--------------------------------
		The Pod that receives this packet will respond, 
			identifying the source IP as its own and 
			the destination IP as the Pod that originally sent the packet 
		Upon entry into the Node
			the packet flows through iptables, 
				which uses conntrack to remember the choice it previously made 
				Rewrite the source of the packet to be the Service’s IP instead of the Pod’s IP 
		Packet flows through the bridge to the virtual Ethernet device 
			paired with the Pod’s namespace
				to the Pod’s Ethernet device as we’ve seen before.

		Using DNS
		---------
		Kubernetes can optionally use DNS to avoid having to hard-code a Service’s cluster IP address into your application.
		-------------------------------------------------------------------------------------------------------------------
		Kubernetes DNS runs as a regular Kubernetes Service 
			that is scheduled on the cluster. 
		It configures the kubelets running on each Node 
			so that containers use the DNS Service’s IP to resolve DNS names. 
		Every Service defined in the cluster (including the DNS server itself) 
			is assigned a DNS name. 
		DNS records resolve DNS names to 
			the cluster IP of the Service or 
			the IP of a POD, 
			depending on your needs. 
			SRV records are used to specify particular named ports for running Services.

		A DNS Pod consists of three separate containers:
		-----------------------------------------------
		kubedns/Coredns: 
			watches the Kubernetes master for changes in Services and Endpoints
			maintains in-memory lookup structures to serve DNS requests.

		dnsmasq: 
			adds DNS caching to improve performance.

		sidecar: 
			provides a single health check endpoint to perform healthchecks for dnsmasq and kubedns.

		The DNS Pod 
			deployed as a Kubernetes Service with a static cluster IP 
			Passed to each running container at startup 
				so that each container can resolve DNS entries. 
		DNS entries are resolved through the kubedns system 
			maintains in-memory DNS representations. 
		etcd is the backend storage system 
			for cluster state, 
		kubedns uses a library that converts 
			etcd key-value stores to 
			DNS entires to rebuild the state of the in-memory DNS lookup structure when necessary.

		-----------------------------------------------------------------------------------------------------
		CoreDNS works similarly to kubedns but is built with a plugin architecture that makes it more flexible. 
		Currently: CoreDNS is the default DNS implementation for Kubernetes.
		-----------------------------------------------------------------------------------------------------

		DNS name for k8s service: service.namespace.svc.cluster.local
		https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/
		https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/
		https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/
		https://www.digitalocean.com/community/tutorials/an-introduction-to-the-kubernetes-dns-service

	Internet-to-Service Networking
	------------------------------

	How to do the following?
	
		1. Getting traffic from a Kubernetes Service out to the Internet
		2. Getting traffic from the Internet to your Kubernetes Service. 
		
		This section deals with each of these concerns in turn.

		6.1 Egress — Routing traffic to the Internet
		--------------------------------------------

		Routing traffic from a Node to the public Internet 
			is network specific 
		Depends on how your network is configured to publish traffic. 
		
-----------------------------------------------------------------------------------------------------------------------
		AWS specific

		for e.g. AWS VPC to discuss any specific details.

In AWS
	a Kubernetes cluster runs within a VPC
	Every Node is assigned a private IP address 
		accessible from within the Kubernetes cluster. 
	To make traffic accessible from outside the cluster
		you attach an Internet gateway to your VPC. 
	The Internet gateway serves two purposes
		provide a target in your VPC route tables for traffic that can be routed to the Internet
		perform (NAT) for any instances that have been assigned public IP addresses
			NATing is responsible for changing the Node’s 
				internal IP address 
					that is private to the cluster 
				to an external IP address 
					that is available in the public Internet.

	With Internet gateway
		VMs are free to route traffic to the Internet. 
		Unfortunately
			Pods have their own IP address 
				not the same as Node IP address
			
			NAT translation at the Internet gateway only works with VM IP 
			the gateway is not container aware. 
	
	
-------------------------------------------------------------------------------------------------------------------------
	How Kubernetes solves this problem using iptables (again).
	----------------------------------------------------------
	Since the solution is in k8s - it should be generic.
	
	Life of a packet: Node to Internet
	----------------------------------
	Packet originates at the Pod’s namespace
	Travels through the veth pair connected to the root namespace
	Once in the root namespace, 
		the packet moves from the bridge to the default device 
			since the IP on the packet does not match any network segment connected to the bridge. 
	Before reaching the root namespace’s Ethernet device 
		iptables mangles the packet 
	In this case, the source IP address of the packet is a Pod, 
		if we keep the source as a Pod 
		the Internet gateway will reject it 
		because the gateway NAT only understands IP addresses 
			that are connected to VMs. 
		solution: iptables perform a source NAT 
			changing the packet source — 
			packet appears to be coming from the VM 
			and not the Pod. 
		With the correct source IP in place, 
			the packet can now leave the VM 
			reach the Internet gateway
		The Internet gateway will do another NAT rewriting the source IP from a VM internal IP to an external IP. 
		Packet will reach the public Internet 
		On the way back, 
			the packet follows the same path and any source IP mangling is undone 
		each layer of the system 
			receives the IP address 
			it understands: VM-internal at the Node or VM level, and a Pod IP within a Pod’s namespace.

----------------------------------------------------
Ingress — Routing Internet traffic to Kubernetes
----------------------------------------------------
Ingress — 
	getting traffic into your cluster — 
	is a surprisingly tricky problem to solve. 
 
This is specific to the network you are running, 
	but in general, 
	Ingress is divided into two solutions that work on different parts of the network stack: 
		1. Service LoadBalancer 
		2. Ingress controller.

	6.2.1 Layer 4 Ingress: LoadBalancer
	-----------------------------------
	When you create a Kubernetes Service you can optionally specify a LoadBalancer to go with it. 
	The implementation of the LoadBalancer is provided by a cloud controller 
		knows how to create a load balancer for your service. 
	Once your Service is created, 
		it will advertise the IP address for the load balancer. 
	As an end user, 
		you can start directing traffic to the load balancer 
			to begin communicating with your Service.

------------------------------------------------------------
	With AWS, 
		load balancers are aware of Nodes 
			within their Target Group and 
			will balance traffic throughout all of the Nodes in the cluster. 
		Once traffic reaches a Node, 
			the iptables rules previously installed throughout the cluster for your Service 
				will ensure that traffic reaches the 
					Pods for the Service you are interested in.
------------------------------------------------------------
	Life of a packet: LoadBalancer to Service
	-----------------------------------------

	Once you deploy your service, 
		a new load balancer will be created for you 
			by the cloud provider you are working with 
			1. Since load balancer is not container aware, 
				once traffic reaches the load-balancer 
					it is distributed throughout the VMs that make up your cluster 
			2. iptables rules on each VM will 
				direct incoming traffic from the load balancer to the correct Pod 
			3 These are the same IP tables rules that were put in place during Service creation and discussed earlier. 
				The response from the Pod to the client will 
					return with the Pod’s IP, 
					but the client needs to have the load balancer’s IP address. 
				iptables and conntrack is used to 
					rewrite the IPs correctly on the return path, 
					as we saw earlier.


	The following diagram shows a network load balancer 
		in front of three VMs 
		that host your Pods. 
	Incoming traffic 
		(1) is directed at the load balancer for your Service. 
			Once the load balancer receives the packet 
		(2) it picks a VM at random. 
		(3). Here, the iptables rules running on the VM will direct the packet to the correct Pod 
			using the internal load balancing rules 
			installed into the cluster using kube-proxy. 
			iptables does the correct NAT and 
			forwards the packet on to the correct Pod 
			(4).

internet-to-service
-------------------
Layer 7 Ingress: Ingress Controller
Layer 7 network Ingress operates on the HTTP/HTTPS protocol range of the network stack and is built on top of Services. The first step to enabling Ingress is to open a port on your Service using the NodePort Service type in Kubernetes. If you set the Service’s type field to NodePort, the Kubernetes master will allocate a port from a range you specify, and each Node will proxy that port (the same port number on every Node) into your Service. That is, any traffic directed to the Node’s port will be forwarded on to the service using iptables rules. This Service to Pod routing follows the same internal cluster load-balancing pattern we’ve already discussed when routing traffic from Services to Pods.

To expose a Node’s port to the Internet you use an Ingress object. An Ingress is a higher-level HTTP load balancer that maps HTTP requests to Kubernetes Services. The Ingress method will be different depending on how it is implemented by the Kubernetes cloud provider controller. HTTP load balancers, like Layer 4 network load balancers, only understand Node IPs (not Pod IPs) so traffic routing similarly leverages the internal load-balancing provided by the iptables rules installed on each Node by kube-proxy.

Within an AWS environment, the ALB Ingress Controller provides Kubernetes Ingress using Amazon’s Layer 7 Application Load Balancer. The following diagram details the AWS components this controller creates. It also demonstrates the route Ingress traffic takes from the ALB to the Kubernetes cluster.


Upon creation, (1) an Ingress Controller watches for Ingress events from the Kubernetes API server. When it finds Ingress resources that satisfy its requirements, it begins the creation of AWS resources. AWS uses an Application Load Balancer (ALB) (2) for Ingress resources. Load balancers work in conjunction with Target Groups that are used to route requests to one or more registered Nodes. (3) Target Groups are created in AWS for each unique Kubernetes Service described by the Ingress resource. (4) A Listener is an ALB process that checks for connection requests using the protocol and port that you configure. Listeners are created by the Ingress controller for every port detailed in your Ingress resource annotations. Lastly, Target Group Rules are created for each path specified in your Ingress resource. This ensures traffic to a specific path is routed to the correct Kubernetes Service (5).

Life of a packet: Ingress to Service
------------------------------------
The life of a packet flowing through an Ingress is very similar to that of a LoadBalancer. The key differences are that an Ingress is aware of the URL’s path (allowing and can route traffic to services based on their path), and that the initial connection between the Ingress and the Node is through the port exposed on the Node for each service.

Let’s look at how this works in practice. Once you deploy your service, a new Ingress load balancer will be created for you by the cloud provider you are working with (1). Because the load balancer is not container aware, once traffic reaches the load-balancer it is distributed throughout the VMs that make up your cluster (2) through the advertised port for your service. iptables rules on each VM will direct incoming traffic from the load balancer to the correct Pod (3) — as we have seen before. The response from the Pod to the client will return with the Pod’s IP, but the client needs to have the load balancer’s IP address. iptables and conntrack is used to rewrite the IPs correctly on the return path, as we saw earlier.

ingress-to-service.gif
One benefit of Layer 7 load-balancers are that they are HTTP aware, so they know about URLs and paths. This allows you you to segment your Service traffic by URL path. They also typically provide the original client’s IP address in the X-Forwarded-For header of the HTTP request.

Wrapping Up
-----------
This guide provides a foundation for understanding the Kubernetes networking model and how it enables common networking tasks. The field of networking is both broad and deep and it’s impossible to cover everything here. This guide should give you a starting point to dive into the topics you are interested in and want to know more about. Whenever you are stumped, leverage the Kubernetes documentation and the Kubernetes community to help you find your way.

8 Glossary
Kubernetes relies on several existing technologies to build a functioning cluster. Fully exploring each of these technologies is beyond the scope of this guide, but this section describes each of those technologies in enough detail to follow along with the discussion. You can feel free to skim this section, skip it completely, or refer to it as needed if you ever get confused or need a refresher.

Layer 2 Networking
-------------------
Layer 2 is the data link layer providing Node-to-Node data transfer. It defines the protocol to establish and terminate a connection between two physically connected devices. It also defines the protocol for flow control between them.

Layer 4 Networking
------------------
The transport layer controls the reliability of a given link through flow control. In TCP/IP, this layer refers to the TCP protocol for exchanging data over an unreliable network.

Layer 7 Networking
------------------
The application layer is the layer closest to the end user, which means both the application layer and the user interact directly with the software application. This layer interacts with software applications that implement a communicating component. Typically, Layer 7 Networking refers to HTTP.

NAT — Network Address Translation
NAT or network address translation is an IP-level remapping of one address space into another. The mapping happens by modifying network address information in the IP header of packets while they are in transit across a traffic routing device.

A basic NAT is a simple mapping from one IP address to another. More commonly, NAT is used to map multiple private IP address into one publicly exposed IP address. Typically, a local network uses a private IP address space and a router on that network is given a private address in that space. The router is then connected to the Internet with a public IP address. As traffic is passed from the local network to the Internet, the source address for each packet is translated from the private address to the public address, making it seem as though the request is coming directly from the router. The router maintains connection tracking to forward replies to the correct private IP on the local network.

NAT provides an additional benefit of allowing large private networks to connect to the Internet using a single public IP address, thereby conserving the number of publicly used IP addresses.

SNAT — Source Network Address Translation
SNAT simply refers to a NAT procedure that modifies the source address of an IP packet. This is the typical behaviour for the NAT described above.

DNAT — Destination Network Address Translation
DNAT refers to a NAT procedure that modifies the destination address of an IP packet. DNAT is used to publish a service resting in a private network to a publicly addressable IP address.

Network Namespace
In networking, each machine (real or virtual) has an Ethernet device (that we will refer to as eth0). All traffic flowing in and out of the machine is associated with that device. In truth, Linux associates each Ethernet device with a network namespace — a logical copy of the entire network stack, with its own routes, firewall rules, and network devices. Initially, all the processes share the same default network namespace from the init process, called the root namespace. By default, a process inherits its network namespace from its parent and so, if you don’t make any changes, all network traffic flows through the Ethernet device specified for the root network namespace.

veth — Virtual Ethernet Device Pairs
Computer systems typically consist of one or more networking devices — eth0, eth1, etc — that are associated with a physical network adapter which is responsible for placing packets onto the physical wire. Veth devices are virtual network devices that are always created in interconnected pairs. They can act as tunnels between network namespaces to create a bridge to a physical network device in another namespace, but can also be used as standalone network devices. You can think of a veth device as a virtual patch cable between devices — what goes in one end will come out the other.

bridge — Network Bridge
A network bridge is a device that creates a single aggregate network from multiple communication networks or network segments. Bridging connects two separate networks as if they were a single network. Bridging uses an internal data structure to record the location that each packet is sent to as a performance optimization.

CIDR — Classless Inter-Domain Routing
CIDR is a method for allocating IP addresses and performing IP routing. With CIDR, IP addresses consist of two groups: the network prefix (which identifies the whole network or subnet), and the host identifier (which specifies a particular interface of a host on that network or subnet). CIDR represents IP addresses using CIDR notation, in which an address or routing prefix is written with a suffix indicating the number of bits of the prefix, such as 192.0.2.0/24 for IPv4. An IP address is part of a CIDR block, and is said to belong to the CIDR block if the initial n bits of the address and the CIDR prefix are the same.

CNI — Container Network Interface
CNI (Container Network Interface) is a Cloud Native Computing Foundation project consisting of a specification and libraries for writing plugins to configure network interfaces in Linux containers. CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted.

VIP — Virtual IP Address
A virtual IP address, or VIP, is a software-defined IP address that doesn’t correspond to an actual physical network interface.

netfilter — The Packet Filtering Framework for Linux
netfilter is the packet filtering framework in Linux. The software implementing this framework is responsible for packet filtering, network address translation (NAT), and other packet mangling.

netfilter, ip_tables, connection tracking (ip_conntrack, nf_conntrack) and the NAT subsystem together build the major parts of the framework.

iptables — Packet Mangling Tool
iptables is a program that allows a Linux system administrator to configure the netfilter and the chains and rules it stores. Each rule within an IP table consists of a number of classifiers (iptables matches) and one connected action (iptables target).

conntrack — Connection Tracking
conntrack is a tool built on top of the Netfilter framework to handle connection tracking. Connection tracking allows the kernel to keep track of all logical network connections or sessions, and direct packets for each connection or session to the correct sender or receiver. NAT relies on this information to translate all related packets in the same way, and iptables can use this information to act as a stateful firewall.

IPVS — IP Virtual Server
IPVS implements transport-layer load balancing as part of the Linux kernel.

IPVS is a tool similar to iptables. It is based on the Linux kernel’s netfilter hook function, but uses a hash table as the underlying data structure. That means, when compared to iptables, IPVS redirects traffic much faster, has much better performance when syncing proxy rules, and provides more load balancing algorithms.

DNS — The Domain Name System
The Domain Name System (DNS) is a decentralized naming system for associating system names with IP addresses. It translates domain names to numerical IP addresses for locating computer services.



#################################################################################################################################
			CALICO BASICS

#################################################################################################################################

https://docs.projectcalico.org/reference/architecture/overview


	Calico architecture
	 
	Calico is made up of the following interdependent components:
		Felix
		The Orchestrator plugin, 
		etcd
		BIRD (https://bird.network.cz/)
		BGP Route Reflector (BIRD),

	Felix, the primary Calico agent that runs on each machine that hosts endpoints.
	The Orchestrator plugin, orchestrator-specific code that tightly integrates Calico into that orchestrator.
	etcd, the data store.
	BIRD, a BGP client that distributes routing information.
	BGP Route Reflector (BIRD), an optional BGP route reflector for higher scale.
	The following sections break down each component in more detail.

	Felix
	Felix is a daemon that runs on every machine that provides endpoints: in most cases that means on nodes that host containers or VMs. It is responsible for programming routes and ACLs, and anything else required on the host, in order to provide the desired connectivity for the endpoints on that host.

	Depending on the specific orchestrator environment, Felix is responsible for the following tasks:

	Interface management
	Felix programs some information about interfaces into the kernel in order to get the kernel to correctly handle the traffic emitted by that endpoint. In particular, it will ensure that the host responds to ARP requests from each workload with the MAC of the host, and will enable IP forwarding for interfaces that it manages.

	It also monitors for interfaces to appear and disappear so that it can ensure that the programming for those interfaces is applied at the appropriate time.

	Route programming
	Felix is responsible for programming routes to the endpoints on its host into the Linux kernel FIB (Forwarding Information Base) . This ensures that packets destined for those endpoints that arrive on at the host are forwarded accordingly.

	ACL programming
	Felix is also responsible for programming ACLs into the Linux kernel. These ACLs are used to ensure that only valid traffic can be sent between endpoints, and ensure that endpoints are not capable of circumventing Calico’s security measures.

	State reporting
	Felix is responsible for providing data about the health of the network. In particular, it reports errors and problems with configuring its host. This data is written into etcd, to make it visible to other components and operators of the network.

	Orchestrator plugin
	Unlike Felix there is no single ‘orchestrator plugin’: instead, there are separate plugins for each major cloud orchestration platform (e.g. OpenStack, Kubernetes). The purpose of these plugins is to bind Calico more tightly into the orchestrator, allowing users to manage the Calico network just as they’d manage network tools that were built into the orchestrator.

	A good example of an orchestrator plugin is the Calico Neutron ML2 mechanism driver. This component integrates with Neutron’s ML2 plugin, and allows users to configure the Calico network by making Neutron API calls. This provides seamless integration with Neutron.

	The orchestrator plugin is responsible for the following tasks:

	API translation
	The orchestrator will inevitably have its own set of APIs for managing networks. The orchestrator plugin’s primary job is to translate those APIs into Calico’s data-model and then store it in Calico’s datastore.

	Some of this translation will be very simple, other bits may be more complex in order to render a single complex operation (e.g. live migration) into the series of simpler operations the rest of the Calico network expects.

	Feedback
	If necessary, the orchestrator plugin will provide feedback from the Calico network into the orchestrator. Examples include: providing information about Felix liveness; marking certain endpoints as failed if network setup failed.

	etcd
	etcd is a distributed key-value store that has a focus on consistency. Calico uses etcd to provide the communication between components and as a consistent data store, which ensures Calico can always build an accurate network.

Depending on the orchestrator plugin, etcd may either be the master data store or a lightweight mirror of a separate data store. For example, in an OpenStack deployment, the OpenStack database is considered the “source of truth” and etcd is used to mirror information about the network to the other Calico components.

The etcd component is distributed across the entire deployment. It is divided into two groups of machines: the core cluster, and the proxies.

For small deployments, the core cluster can be an etcd cluster of one node (which would typically be co-located with the orchestrator plugin component). This deployment model is simple but provides no redundancy for etcd – in the case of etcd failure the orchstrator plugin would have to rebuild the database which, as noted for OpenStack, will simply require that the plugin resynchronizes state to etcd from the OpenStack database.

In larger deployments, the core cluster can be scaled up, as per the etcd admin guide.

Additionally, on each machine that hosts either a Felix or a plugin, we run an etcd proxy. This reduces the load on the core cluster and shields nodes from the specifics of the etcd cluster. In the case where the etcd cluster has a member on the same machine as a plugin, we can forgo the proxy on that machine.

etcd is responsible for performing the following tasks:

Data storage
etcd stores the data for the Calico network in a distributed, consistent, fault-tolerant manner (for cluster sizes of at least three etcd nodes). This set of properties ensures that the Calico network is always in a known-good state, while allowing for some number of the machines hosting etcd to fail or become unreachable.

This distributed storage of Calico data also improves the ability of the Calico components to read from the database (which is their most common operation), as they can distribute their reads around the cluster.

Communication
etcd is also used as a communication bus between components. We do this by having the non-etcd components watch certain points in the keyspace to ensure that they see any changes that have been made, allowing them to respond to those changes in a timely manner. This allows the act of committing the state to the database to cause that state to be programmed into the network.

BGP client (BIRD)
Calico deploys a BGP client on every node that also hosts a Felix. The role of the BGP client is to read routing state that Felix programs into the kernel and distribute it around the data center.

The BGP client is responsible for performing the following task:

Route distribution
When Felix inserts routes into the Linux kernel FIB, the BGP client will pick them up and distribute them to the other nodes in the deployment. This ensures that traffic is efficiently routed around the deployment.

BGP route reflector (BIRD)
For larger deployments, simple BGP can become a limiting factor because it requires every BGP client to be connected to every other BGP client in a mesh topology. This requires an increasing number of connections that rapidly become tricky to maintain, due to the N^2 nature of the increase.

For that reason, in larger deployments, Calico will deploy a BGP route reflector. This component, commonly used in the Internet, acts as a central point to which the BGP clients connect, preventing them from needing to talk to every single BGP client in the cluster.

For redundancy, multiple BGP route reflectors can be deployed seamlessly. The route reflectors are purely involved in the control of the network: no endpoint data passes through them.

In Calico, this BGP component is also most commonly BIRD, configured as a route reflector rather than as a standard BGP client.

The BGP route reflector is responsible for the following task:

Centralized route distribution
When the Calico BGP client advertises routes from its FIB to the route reflector, the route reflector advertises those routes out to the other nodes in the deployment.

#################################################################################################################################
			CALICO BASICS ENDS

#################################################################################################################################
